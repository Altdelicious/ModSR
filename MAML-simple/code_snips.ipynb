{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.checkpoint as cp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from scipy.io import savemat\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import time\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "from meta import MetaLearner\n",
    "from naive import VDSR_mod\n",
    "from dataGenerator import dataGenerator\n",
    "from utils import convert_rgb_to_ycbcr, convert_ycbcr_to_rgb, calc_psnr, calc_ssim, AverageMeter, GradLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-traveler",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../../data/DIV2K_train_HR'\n",
    "weights_file = '../VDSR-pytorch/pretrained_models/vdsr_x2.pth'\n",
    "meta_batch_size = 3\n",
    "meta_lr = 1e-3\n",
    "num_updates = 5\n",
    "task_batch_size = 5\n",
    "crop_size = 96\n",
    "scale = 2\n",
    "batch_size = meta_batch_size*task_batch_size*2\n",
    "device0, device1 = 'cuda:0', 'cuda:1'\n",
    "\n",
    "train_set = dataGenerator(dataset_dir, meta_batch_size, task_batch_size, crop_size, scale)\n",
    "meta = MetaLearner(VDSR_mod, weights_file, meta_batchsz=meta_batch_size, beta=meta_lr, num_updates=num_updates,\n",
    "                   device0=device0, device1=device1)\n",
    "\n",
    "for episode_num in range(1000):\n",
    "    inputa, labela, inputb, labelb = train_set.__getitem__()\n",
    "    inputa, inputb = inputa.to(device0), inputb.to(device0)\n",
    "    labela, labelb = labela.to(device0), labelb.to(device0)\n",
    "    \n",
    "    loss = meta(inputa, labela, inputb, labelb)\n",
    "    print('episode_num:{}, dummy-loss:{:.4f}'.format(episode_num, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-distinction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-imagination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-management",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #meta_batchsz = 32\n",
    "    #n_way = 5\n",
    "    #k_shot = 1\n",
    "    #k_query = k_shot\n",
    "    #meta_lr = 1e-3\n",
    "    #num_updates = 5\n",
    "    #dataset = 'omniglot'\n",
    "\n",
    "\n",
    "\n",
    "    #if dataset == 'omniglot':\n",
    "    #    imgsz = 28\n",
    "    #    db = OmniglotNShot('dataset', batchsz=meta_batchsz, n_way=n_way, k_shot=k_shot, k_query=k_query, imgsz=imgsz)\n",
    "\n",
    "    #elif dataset == 'mini-imagenet':\n",
    "    #    imgsz = 84\n",
    "    #    # the dataset loaders are different from omniglot to mini-imagenet. for omniglot, it just has one loader to use\n",
    "    #    # get_batch(train or test) to get different batch.\n",
    "    #    # for mini-imagenet, it should have two dataloader, one is train_loader and another is test_loader.\n",
    "    #    mini = MiniImagenet('../mini-imagenet/', mode='train', n_way=n_way, k_shot=k_shot, k_query=k_query,\n",
    "    #                        batchsz=10000, resize=imgsz)\n",
    "    #    db = DataLoader(mini, meta_batchsz, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    #    mini_test = MiniImagenet('../mini-imagenet/', mode='test', n_way=n_way, k_shot=k_shot, k_query=k_query,\n",
    "    #                        batchsz=1000, resize=imgsz)\n",
    "    #    db_test = DataLoader(mini_test, meta_batchsz, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    #else:\n",
    "    #    raise  NotImplementedError\n",
    "    \n",
    "    \n",
    "    #meta = MetaLearner(Naive, meta_batchsz=meta_batchsz, beta=meta_lr,\n",
    "                       num_updates=num_updates).cuda()\n",
    "    \n",
    "    tb = SummaryWriter('runs')\n",
    "\n",
    "\n",
    "    # main loop\n",
    "    for episode_num in range(200000):\n",
    "\n",
    "        # 1. train\n",
    "        if dataset == 'omniglot':\n",
    "            support_x, support_y, query_x, query_y = db.get_batch('test')\n",
    "            support_x = Variable( torch.from_numpy(support_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "            query_x = Variable( torch.from_numpy(query_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "            support_y = Variable(torch.from_numpy(support_y).long()).cuda()\n",
    "            query_y = Variable(torch.from_numpy(query_y).long()).cuda()\n",
    "        elif dataset == 'mini-imagenet':\n",
    "            try:\n",
    "                batch_test = iter(db).next()\n",
    "            except StopIteration as err:\n",
    "                mini = MiniImagenet('../mini-imagenet/', mode='train', n_way=n_way, k_shot=k_shot, k_query=k_query,\n",
    "                                    batchsz=10000, resize=imgsz)\n",
    "                db = DataLoader(mini, meta_batchsz, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "            support_x = Variable(batch_test[0]).cuda()\n",
    "            support_y = Variable(batch_test[1]).cuda()\n",
    "            query_x = Variable(batch_test[2]).cuda()\n",
    "            query_y = Variable(batch_test[3]).cuda()\n",
    "\n",
    "        # backprop has been embedded in forward func.\n",
    "        accs = meta(support_x, support_y, query_x, query_y)\n",
    "        train_acc = np.array(accs).mean()\n",
    "\n",
    "        # 2. test\n",
    "        if episode_num % 30 == 0:\n",
    "            test_accs = []\n",
    "            for i in range(min(episode_num // 5000 + 3, 10)): # get average acc.\n",
    "                if dataset == 'omniglot':\n",
    "                    support_x, support_y, query_x, query_y = db.get_batch('test')\n",
    "                    support_x = Variable( torch.from_numpy(support_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "                    query_x = Variable( torch.from_numpy(query_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "                    support_y = Variable(torch.from_numpy(support_y).long()).cuda()\n",
    "                    query_y = Variable(torch.from_numpy(query_y).long()).cuda()\n",
    "                elif dataset == 'mini-imagenet':\n",
    "                    try:\n",
    "                        batch_test = iter(db_test).next()\n",
    "                    except StopIteration as err:\n",
    "                        mini_test = MiniImagenet('../mini-imagenet/', mode='test', n_way=n_way, k_shot=k_shot,\n",
    "                                                 k_query=k_query,\n",
    "                                                 batchsz=1000, resize=imgsz)\n",
    "                        db_test = DataLoader(mini_test, meta_batchsz, shuffle=True, num_workers=2, pin_memory=True)\n",
    "                    support_x = Variable(batch_test[0]).cuda()\n",
    "                    support_y = Variable(batch_test[1]).cuda()\n",
    "                    query_x = Variable(batch_test[2]).cuda()\n",
    "                    query_y = Variable(batch_test[3]).cuda()\n",
    "\n",
    "\n",
    "                # get accuracy\n",
    "                test_acc = meta.pred(support_x, support_y, query_x, query_y)\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "            test_acc = np.array(test_accs).mean()\n",
    "            print('episode:', episode_num, '\\tfinetune acc:%.6f' % train_acc, '\\t\\ttest acc:%.6f' % test_acc)\n",
    "            tb.add_scalar('test-acc', test_acc)\n",
    "            tb.add_scalar('finetune-acc', train_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-deployment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-program",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-dining",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
